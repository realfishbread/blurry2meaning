import torch
import torch.nn as nn
from torchvision.transforms import ToTensor, Resize, ToPILImage
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from PIL import ImageEnhance

# 1. SRCNN ëª¨ë¸ ì •ì˜
class FSRCNN(nn.Module):
    def __init__(self, scale_factor=2):
        super(FSRCNN, self).__init__()
        self.feature_extraction = nn.Conv2d(3, 56, kernel_size=5, padding=2)
        self.shrinking = nn.Conv2d(56, 12, kernel_size=1)
        self.mapping = nn.Sequential(
            nn.Conv2d(12, 12, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(12, 12, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(12, 12, kernel_size=3, padding=1)
        )
        self.expanding = nn.Conv2d(12, 56, kernel_size=1)
        self.deconv = nn.ConvTranspose2d(
    56, 3,
    kernel_size=8,   # ì¡°ê¸ˆ ì‘ê²Œ
    stride=2,
    padding=3,
    output_padding=0  # ê°€ëŠ¥í•˜ë©´ ì´ê±¸ë¡œ ì¤„ë¬´ëŠ¬ ì—†ì•°
)

    def forward(self, x):
        x = torch.relu(self.feature_extraction(x))
        x = torch.relu(self.shrinking(x))
        x = self.mapping(x)
        x = torch.relu(self.expanding(x))
        x = self.deconv(x)
        return x

# 2. ëª¨ë¸ ë¡œë”©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FSRCNN().to(device)  # âœ… ì´ê²Œ ë§ì•„!
model.load_state_dict(torch.load("results/fsrcnn_model.pth", map_location=device))

model.eval()

# 3. ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
image = Image.open("user_input_lowres.jpg").convert("RGB")
image = image.resize((32, 32))  # CIFAR ìŠ¤íƒ€ì¼ í¬ê¸°
lowres_tensor = ToTensor()(image).unsqueeze(0).to(device)


enhancer = ImageEnhance.Brightness(image)
image = enhancer.enhance(1.5)  # ë°ê¸° 1.5ë°°

# 4. ë³µì›
with torch.no_grad():
    restored_tensor = model(lowres_tensor).cpu()

# âœ… ë¨¼ì € clampë¡œ í”½ì…€ ê°’ì„ [0, 1]ë¡œ ì œí•œí•´ì¤˜ì•¼ í•¨
restored_tensor = torch.clamp(restored_tensor, 0, 1)

# âœ… ê·¸ ë‹¤ìŒì— PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
restored_image = ToPILImage()(restored_tensor.squeeze(0))

# âœ… ì €ì¥
restored_image.save("restored_output.jpg")

# 5. CLIP ëª¨ë¸ ë¡œë”© (ë©€í‹°ëª¨ë‹¬ ë¶„ë¥˜ê¸°)
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 6. ì¶”ë¡  í…ìŠ¤íŠ¸ í›„ë³´
labels = [
    "a sports car", "a dark car", "a toy car", "a blurry car", "a ship", "a motorcycle",
    "a truck", "a spaceship"
]

# 7. ì¶”ë¡  ì‹¤í–‰
inputs = processor(text=labels, images=restored_image, return_tensors="pt", padding=True)
with torch.no_grad():
    outputs = clip_model(**inputs)
    probs = outputs.logits_per_image.softmax(dim=1)

top_idx = torch.argmax(probs)
confidence = probs[0][top_idx].item()
label = labels[top_idx]

print(f"ğŸ’¡ AIì˜ ì¶”ë¡ : '{label}' ({confidence*100:.2f}%)")